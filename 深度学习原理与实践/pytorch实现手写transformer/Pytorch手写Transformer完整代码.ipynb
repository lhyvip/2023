{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dd4c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch手写Transformer完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b337052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_idx2word= {0: 'p', 1: '我', 2: '有', 3: '一', 4: '个', 5: '好', 6: '朋', 7: '友', 8: '零', 9: '女'}\n",
      "idx2word= {0: 'p', 1: 'i', 2: 'have', 3: 'a', 4: 'good', 5: 'friend', 6: 'zero', 7: 'girl', 8: 'S', 9: 'E', 10: '.'}\n"
     ]
    }
   ],
   "source": [
    "#1.数据构建\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device=\"cpu\"\n",
    "#device=\"cuda\"\n",
    "\n",
    "#transformer epochs\n",
    "epochs=10\n",
    "\n",
    "# 这里我没有用什么大型的数据集，而是手动输入了两对中文→英语的句子\n",
    "# 还有每个字的索引也是我手动硬编码上去的，主要是为了降低代码阅读难度\n",
    "# S: 代表开始Symbol that shows starting of decoding input\n",
    "# E: 代表结束Symbol that shows starting of decoding output\n",
    "# P: 用于填充Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "\n",
    "#训练集\n",
    "sentences=[\n",
    "    #中文和英文的单词个数不要求相同\n",
    "    #enc_input                 dec_input                 dec_output\n",
    "    [\"我 有 一 个 好 朋 友 p\",\"S i have a good friend .\",\"i have a good friend . E\"],\n",
    "    [\"我 有 零 个 女 朋 友 p\",\"S i have zero girl friend .\",\"i have zero girl friend . E\"]  \n",
    "]\n",
    "\n",
    "#测试集（期望transformers能达到的一个效果是）\n",
    "#输入 “我有一个女朋友”\n",
    "#输出“i have a girl friend”\n",
    "\n",
    "#中文和英语的单词要分开建立词库\n",
    "#padding should be Zero填充应该对应于0\n",
    "src_vocab={\"p\":0,\"我\":1,\"有\":2,\"一\":3,\"个\":4,\"好\":5\n",
    "           ,\"朋\":6,\"友\":7,\"零\":8,\"女\":9}\n",
    "\n",
    "src_idx2word={i : w for i ,w in enumerate(src_vocab)}\n",
    "print(\"src_idx2word=\",src_idx2word)\n",
    "src_vocab_size=len(src_vocab)\n",
    "\n",
    "tgt_vocab={\"p\":0,\"i\":1,\"have\":2,\"a\":3,\"good\":4,\"friend\":5\n",
    "           ,\"zero\":6,\"girl\":7,\"S\":8,\"E\":9,\".\":10}\n",
    "idx2word={i : w for i,w in enumerate(tgt_vocab)}\n",
    "tgt_vocab_size=len(tgt_vocab)\n",
    "print(\"idx2word=\",idx2word)\n",
    "\n",
    "src_len=8 #编码器输入的最大单词数量是8\n",
    "tgt_len=7#解码器输入=输出的最大的词量等于7\n",
    "\n",
    "\n",
    "#transformer pamameters超参数\n",
    "d_model=512 # Embedding Size（token embedding和position编码的维度）\n",
    "d_ff=2048#两次线性层中的隐藏层 512->2048->512线性层是用来做特征提取的）\n",
    "#，当然最后会再接一个projection层\n",
    "d_k=d_v=64# dimension of K(=Q), V\n",
    "#（Q和K的维度需要相同，这里为了方便让K=V）\n",
    "n_layers=6#编码器encoder与解码器decoder每个模块儿循环的次数Nx\n",
    "n_heads=8#number of heads in Multi-Head Attention（多头注意力机制的头）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "487067d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_inputs= tensor([[1, 2, 3, 4, 5, 6, 7, 0],\n",
      "        [1, 2, 8, 4, 9, 6, 7, 0]])\n",
      "dec_inputs= tensor([[ 8,  1,  2,  3,  4,  5, 10],\n",
      "        [ 8,  1,  2,  6,  7,  5, 10]])\n",
      "dec_outputs= tensor([[ 1,  2,  3,  4,  5, 10,  9],\n",
      "        [ 1,  2,  6,  7,  5, 10,  9]])\n"
     ]
    }
   ],
   "source": [
    "#2.数据构建\n",
    "def make_data(sentences):\n",
    "    \"\"\"把单词的序列转化为数字的序列\"\"\"\n",
    "    enc_inputs,dec_inputs,dec_outputs=[],[],[]\n",
    "    for i in range (len(sentences)):\n",
    "        enc_input=[[src_vocab[n] for n in sentences[i][0].split()]]\n",
    "        # tensor([[1, 2, 3, 4, 5, 6, 7, 0],[1, 2, 8, 4, 9, 6, 7, 0]])\n",
    "        dec_input=[[tgt_vocab[n] for n in sentences[i][1].split()]]\n",
    "        #tensor([[ 8,  1,  2,  3,  4,  5, 10], [ 8,  1,  2,  6,  7,  5, 10]])\n",
    "        dec_output=[[tgt_vocab[n] for n in sentences[i][2].split()]]\n",
    "        #tensor([[ 1,  2,  3,  4,  5, 10,  9],[ 1,  2,  6,  7,  5, 10,  9]])\n",
    "        \n",
    "        enc_inputs.extend(enc_input)\n",
    "        dec_inputs.extend(dec_input)\n",
    "        dec_outputs.extend(dec_output)\n",
    "    return torch.LongTensor(enc_inputs),torch.LongTensor(dec_inputs),torch.LongTensor(dec_outputs)\n",
    "\n",
    "\n",
    "enc_inputs,dec_inputs,dec_outputs=make_data(sentences=sentences)    \n",
    "print(\"enc_inputs=\",enc_inputs)\n",
    "print(\"dec_inputs=\",dec_inputs)\n",
    "print(\"dec_outputs=\",dec_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2af5c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.自定义DataLoader\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self,enc_inputs,dec_inputs,dec_outputs):\n",
    "        super(MyDataSet,self).__init__()\n",
    "        self.enc_inputs=enc_inputs\n",
    "        self.dec_inputs=dec_inputs\n",
    "        self.dec_outputs=dec_outputs\n",
    "    def __len__(self):\n",
    "        return self.enc_inputs.shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        return self.enc_inputs[idx],self.dec_inputs[idx],self.dec_outputs[idx]\n",
    "    \n",
    "\n",
    "loader = DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f108331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.Transformer模型\n",
    "\n",
    "#4.1位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model,dropout=0.1,max_len=5000):\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        self.dropout=nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe=torch.zeros(max_len,d_model)#（5000,512）\n",
    "        position=torch.arange(0,max_len,dtype=torch.float).unsqueeze(1)\n",
    "        #(5000,1)\n",
    "        div_term=torch.exp(torch.arange(0,d_model,2).float()*(-math.log(10000.0)/d_model))\n",
    "        #(256)\n",
    "        pe[:,0::2]=torch.sin(position*div_term)#（5000,256）\n",
    "        pe[:,1::2]=torch.cos(position*div_term)#（5000,256）\n",
    "        print(\"pe=\",pe.shape)\n",
    "        pe=pe.unsqueeze(0).transpose(0,1)#（1,5000,512）\n",
    "        print(\"pe=\",pe.shape)\n",
    "        self.register_buffer(\"pe\",pe)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #x:[seq_len,batch_size,d_model]\n",
    "        x=x+self.pe[:x.size(0),:]\n",
    "        return self.dropout(x)\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f14ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2  # pad mask的作用：在对value向量加权平均的时候，\n",
    "#可以让pad对应的alpha_ij=0，\n",
    "#这样注意力就不会考虑到pad向量\n",
    "def get_attn_pad_mask(seq_q,seq_k):\n",
    "    # pad mask的作用：在对value向量加权平均的时候，可以让pad对应的alpha_ij=0，这样注意力就不会考虑到pad向量\n",
    "    \"\"\"这里的q,k表示的是两个序列（跟注意力机制的q,k没有关系），例如encoder_inputs (x1,x2,..xm)和encoder_inputs (x1,x2..xm)\n",
    "    encoder和decoder都可能调用这个函数，所以seq_len视情况而定\n",
    "    seq_q: [batch_size, seq_len]\n",
    "    seq_k: [batch_size, seq_len]\n",
    "    seq_len could be src_len or it could be tgt_len\n",
    "    seq_len in seq_q and seq_len in seq_k maybe not equal\n",
    "    \"\"\"\n",
    "    batch_size, len_q = seq_q.size()  # 这个seq_q只是用来expand维度的\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    # 例如:seq_k = [[1,2,3,4,0], [1,2,3,5,0]]\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # [batch_size, 1, len_k], True is masked\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # [batch_size, len_q, len_k] 构成一个立方体(batch_size个这样的矩阵)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa0534c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.3 \n",
    "def get_attn_subsequence_mask(seq):\n",
    "    \"\"\"建议打印出来看看是什么的输出（一目了然）\n",
    "    seq: [batch_size, tgt_len]\n",
    "    \"\"\"\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    # attn_shape: [batch_size, tgt_len, tgt_len]\n",
    "    subsequence_mask = np.triu(np.ones(attn_shape), k=1)  # 生成一个上三角矩阵\n",
    "    subsequence_mask = torch.from_numpy(subsequence_mask).byte()\n",
    "    return subsequence_mask  # [batch_size, tgt_len, tgt_len]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cbcbff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention,self).__init__()\n",
    "    def forward(self,Q,K,V,attn_mask):\n",
    "        \"\"\"\n",
    "        Q: [batch_size, n_heads, len_q, d_k]\n",
    "        K: [batch_size, n_heads, len_k, d_k]\n",
    "        V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "        attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
    "        说明：在encoder-decoder的Attention层中len_q(q1,..qt)和len_k(k1,...km)可能不同\n",
    "        \"\"\"\n",
    "        scores=torch.matmul(Q,K.transpose(-1,-2)/np.sqrt(d_k))\n",
    "        # scores : [batch_size, n_heads, len_q, len_k]\n",
    "        \n",
    "        # mask矩阵填充scores（用-1e9填充scores\n",
    "        #中与attn_mask中值为1位置相对应的元素）Ll\n",
    "        scores.masked_fill_(attn_mask,-1e9)\n",
    "        attn=nn.Softmax(dim=-1)(scores)# 对最后一个维度(v)做softmax\n",
    "        # scores : [batch_size, n_heads, len_q, len_k] * V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "        \n",
    "        context=torch.matmul(attn,V)# context: [batch_size, n_heads, len_q, d_v]\n",
    "        # context：[[z1,z2,...],[...]]向量, attn注意力稀疏矩阵（用于可视化的）\n",
    "        return context,attn\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78d7e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.5 \n",
    "\"\"\"这个Attention类可以实现:\n",
    "    Encoder的Self-Attention\n",
    "    Decoder的Masked Self-Attention\n",
    "    Encoder-Decoder的Attention\n",
    "    输入：seq_len x d_model\n",
    "    输出：seq_len x d_model\n",
    "    \"\"\"\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.W_Q=nn.Linear(d_model,d_k*n_heads,bias=False)\n",
    "        #(512,64*8)# q,k必须维度相同，不然无法做点积\n",
    "        self.W_K=nn.Linear(d_model,d_k*n_heads,bias=False)\n",
    "        self.W_V=nn.Linear(d_model,d_v*n_heads,bias=False)\n",
    "        # 这个全连接层可以保证多头attention的输出仍然是seq_len x d_model\n",
    "        self.fc=nn.Linear(n_heads*d_v,d_model,bias=False)\n",
    "        \n",
    "    def forward(self,input_Q,input_K,input_V,attn_mask):\n",
    "        \"\"\"\n",
    "        input_Q: [batch_size, len_q, d_model]\n",
    "        input_K: [batch_size, len_k, d_model]\n",
    "        input_V: [batch_size, len_v(=len_k), d_model]\n",
    "        attn_mask: [batch_size, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        residual,batch_size=input_Q,input_Q.size(0)\n",
    "        # 下面的多头的参数矩阵是放在一起做线性变换的，\n",
    "        #然后再拆成多个头，这是工程实现的技巧\n",
    "        # B: batch_size, S:seq_len, D: dim\n",
    "        # (B, S, D) -proj-> (B, S, D_new) -split\n",
    "        #-> (B, S, Head, W) -trans-> (B, Head, S, W)\n",
    "        #  线性变换          拆成多头\n",
    "        \n",
    "        # Q: [batch_size, n_heads, len_q, d_k]\n",
    "        Q=self.W_Q(input_Q).view(batch_size,-1,n_heads,d_k).transpose(1,2)\n",
    "        \n",
    "        # K: [batch_size, n_heads, len_k, d_k] \n",
    "        # K和V的长度一定相同，维度可以不同\n",
    "        K=self.W_K(input_K).view(batch_size,-1,n_heads,d_k).transpose(1,2)\n",
    "        \n",
    "        # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "        V = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1, 2)\n",
    "        # 因为是多头，所以mask矩阵要扩充成4维的\n",
    "        # attn_mask: [batch_size, seq_len, seq_len] \n",
    "        #-> [batch_size, n_heads, seq_len, seq_len]\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "        # context: [batch_size, n_heads, len_q, d_v], \n",
    "        #attn: [batch_size, n_heads, len_q, len_k]\n",
    "        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)\n",
    "        # 下面将不同头的输出向量拼接在一起\n",
    "        # context: [batch_size, n_heads, len_q, d_v] -> [batch_size, len_q, n_heads * d_v]\n",
    "        context=context.transpose(1,2).reshape(batch_size,-1,n_heads*d_v)\n",
    "        # 这个全连接层可以保证多头attention的输出仍然是seq_len x d_model\n",
    "        output=self.fc(context) # [batch_size, len_q, d_model]\n",
    "        return nn.LayerNorm(d_model).to(device)(output+residual),attn\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "598641d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch中的Linear只会对最后一维操作，所以正好是我们希望的每个位置用同一个全连接网络\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet,self).__init__()\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(d_model,d_ff,bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff,d_model,bias=False)\n",
    "        \n",
    "        )\n",
    "    def forward(self,inputs):\n",
    "        \n",
    "        \"\"\"\n",
    "        inputs: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        residual=inputs\n",
    "        output=self.fc(inputs)\n",
    "        return nn.LayerNorm(d_model).to(device)(output+residual)# [batch_size, seq_len, d_model]\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "907c2d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.enc_self_attn=MultiHeadAttention()\n",
    "        self.pos_ffn=PoswiseFeedForwardNet()\n",
    "    def forward(self,enc_inputs,enc_self_attn_mask):\n",
    "        \"\"\"E\n",
    "        enc_inputs: [batch_size, src_len, d_model]\n",
    "        enc_self_attn_mask: [batch_size, src_len, src_len] \n",
    "        mask矩阵(pad mask or sequence mask)\n",
    "        \"\"\"\n",
    "        # enc_outputs: [batch_size, src_len, d_model], \n",
    "        #attn: [batch_size, n_heads, src_len, src_len]\n",
    "        # 第一个enc_inputs * W_Q = Q\n",
    "        # 第二个enc_inputs * W_K = K\n",
    "        # 第三个enc_inputs * W_V = V\n",
    "        enc_outputs,attn=self.enc_self_attn(enc_inputs,enc_inputs,enc_inputs,enc_self_attn_mask)\n",
    "        # enc_inputs to same Q,K,V（未线性变换前）\n",
    "        \n",
    "        enc_outputs=self.pos_ffn(enc_outputs)# enc_outputs: [batch_size, src_len, d_model]\n",
    "        return enc_outputs,attn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17c939ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.dec_self_attn=MultiHeadAttention()\n",
    "        self.dec_enc_attn=MultiHeadAttention()\n",
    "        self.pos_ffn=PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
    "        \"\"\"\n",
    "        dec_inputs: [batch_size, tgt_len, d_model]\n",
    "        enc_outputs: [batch_size, src_len, d_model]\n",
    "        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\n",
    "        dec_enc_attn_mask: [batch_size, tgt_len, src_len]\n",
    "        \"\"\"\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]\n",
    "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs,\n",
    "                                                        dec_self_attn_mask)  # 这里的Q,K,V全是Decoder自己的输入\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\n",
    "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs,\n",
    "                                                      dec_enc_attn_mask)  # Attention层的Q(来自decoder) 和 K,V(来自encoder)\n",
    "        dec_outputs = self.pos_ffn(dec_outputs)  # [batch_size, tgt_len, d_model]\n",
    "        return dec_outputs, dec_self_attn, dec_enc_attn  # dec_self_attn, dec_enc_attn这两个是为了可视化的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb5480d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.src_emb=nn.Embedding(src_vocab_size,d_model)#进行emding词编码\n",
    "        self.pos_emb=PositionalEncoding(d_model)#transformer中位置编码固定的不需要学习\n",
    "        self.layers=nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        \n",
    "    def forward(self,enc_inputs):\n",
    "        \"\"\"\n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        \"\"\"\n",
    "        #print(\"============Encoder=========\")\n",
    "       # print(\"enc_inputs=\",enc_inputs.shape)#1,8\n",
    "        enc_outputs=self.src_emb(enc_inputs)\n",
    "        enc_outputs=self.pos_emb((enc_outputs.transpose(0,1)).transpose(0,1))\n",
    "        # [batch_size, src_len, d_model]   \n",
    "        # Encoder输入序列的pad mask矩阵\n",
    "        ens_self_attn_mask=get_attn_pad_mask(enc_inputs,enc_inputs)\n",
    "         # [batch_size, src_len, src_len]\n",
    "        enc_self_attns = []  # 在计算中不需要用到，\n",
    "        #它主要用来保存你接下来返回的attention的值（这个主要是为了你画热力图等，\n",
    "        #用来看各个词之间的关系\n",
    "        for layer in self.layers:\n",
    "            # for循环访问nn.ModuleList对象\n",
    "            # 上一个block的输出enc_outputs作为当前block的输入\n",
    "            # enc_outputs: [batch_size, src_len, d_model], \n",
    "            #enc_self_attn: [batch_size, n_heads, src_len, src_len]\n",
    "            enc_outputs,enc_self_attn=layer(enc_outputs,ens_self_attn_mask)\n",
    "            # 传入的enc_outputs其实是input，传入mask矩阵是因为你要做self attention\n",
    "            enc_self_attns.append(enc_self_attn)\n",
    "        return enc_outputs,enc_self_attns\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "916afb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.tgt_emb=nn.Embedding(tgt_vocab_size, d_model)# Decoder输入的embed词表\n",
    "        self.pos_emb=PositionalEncoding(d_model)\n",
    "        self.layers=nn.ModuleList([DecoderLayer()for _ in range(n_layers)])#Decoderblocks\n",
    "    def forward(self,dec_inputs,enc_inputs,enc_outputs):\n",
    "        #print(\"==========Decoder=========\")\n",
    "        \"\"\"\n",
    "        dec_inputs: [batch_size, tgt_len]1,7\n",
    "        enc_inputs: [batch_size, src_len]1,8\n",
    "        enc_outputs: [batch_size，src_len，d_model]1,8,512\n",
    "        用在Encoder-Decoder Attention层un\n",
    "        \"\"\"\n",
    "        dec_outputs=self.tgt_emb(dec_inputs)#[batch_size,tgt_len,d_model]\n",
    "        #print(\"dec_outputs=\",dec_outputs.shape)#1,7,512\n",
    "        dec_outputs=self.pos_emb(dec_outputs.transpose(0,1).transpose(0,1).to(device))\n",
    "        #[batch_size,tgt_len,d_model]\n",
    "        #Decoder输入序列的pad mask矩阵（这个例子中decoder是没有加pad的实际应用中都有pad填充的）\n",
    "        dec_self_attn_pad_mask=get_attn_pad_mask(dec_inputs,dec_inputs).to(device)\n",
    "        dec_self_attn_pad_subsequence_mask=get_attn_subsequence_mask(dec_inputs).to(device)\n",
    "        dec_self_attn_mask=torch.gt((dec_self_attn_pad_mask+dec_self_attn_pad_subsequence_mask),0).to(device)\n",
    "        dec_enc_attns_mask=get_attn_pad_mask(dec_inputs,enc_inputs)\n",
    "        dec_self_attns,dec_enc_attns=[],[]\n",
    "        for layer in self.layers:\n",
    "            dec_outputs,dec_self_attn,dec_enc_attn=layer(dec_outputs,enc_outputs,dec_self_attn_mask,dec_enc_attns_mask)\n",
    "            dec_self_attns.append(dec_self_attn)\n",
    "            dec_enc_attns.append(dec_enc_attns)\n",
    "                                                         \n",
    "                                                         \n",
    "        return dec_outputs,dec_self_attns,dec_enc_attns\n",
    "                                                         \n",
    "                                                         \n",
    "                                                         \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa8f7b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pe= torch.Size([5000, 512])\n",
      "pe= torch.Size([5000, 1, 512])\n",
      "pe= torch.Size([5000, 512])\n",
      "pe= torch.Size([5000, 1, 512])\n",
      "Epoch:  0001 loss = 2.515517\n",
      "Epoch:  0001 loss = 2.478357\n",
      "Epoch:  0002 loss = 2.331035\n",
      "Epoch:  0002 loss = 2.146213\n",
      "Epoch:  0003 loss = 1.946074\n",
      "Epoch:  0003 loss = 1.716435\n",
      "Epoch:  0004 loss = 1.606998\n",
      "Epoch:  0004 loss = 1.345000\n",
      "Epoch:  0005 loss = 1.070809\n",
      "Epoch:  0005 loss = 1.007999\n",
      "Epoch:  0006 loss = 0.834144\n",
      "Epoch:  0006 loss = 0.518203\n",
      "Epoch:  0007 loss = 0.405041\n",
      "Epoch:  0007 loss = 0.413458\n",
      "Epoch:  0008 loss = 0.233885\n",
      "Epoch:  0008 loss = 0.234522\n",
      "Epoch:  0009 loss = 0.177087\n",
      "Epoch:  0009 loss = 0.147307\n",
      "Epoch:  0010 loss = 0.096143\n",
      "Epoch:  0010 loss = 0.184499\n"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer,self).__init__()\n",
    "        self.encoder=Encoder().to(device)\n",
    "        self.decoder=Decoder().to(device)\n",
    "        self.projection=nn.Linear(d_model,tgt_vocab_size,bias=False).to(device)\n",
    "    def forward(self,enc_inputs,dec_inputs):\n",
    "        \"\"\"Transformers的输入:两个序列\n",
    "        enc_inputs: [batch_size, src_len](1,8)\n",
    "        dec_inputs: [batch size, tgt_len](1,7)\n",
    "        tgt_vocab_size  11\n",
    "        \"\"\"\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        # outputs = torch.zeros(batch_size，tgt_len，tgt_vocab_size).to(self.device)\n",
    "        # enc_outputs: [batch_size，src_len，d_model],\n",
    "        #enc_self_attns: [n_layers，batch size,n_heads，src_len，src_lenJ\n",
    "        # 经过Encoder网络后，得到的输出还是[batch_size, src_len,d_model]\n",
    "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
    "        #print(\"enc_outputs=\",enc_outputs.shape)#(1,8,512)\n",
    "        # dec_outputs: [batch_size，tgt_len，d_model],\n",
    "        #dec_self_attns: [n_layers， batch size，n_heads， tgt_len， tgt_len)\n",
    "        dec_outputs,dec_self_attns,dec_enc_attns= self.decoder(dec_inputs,enc_inputs,enc_outputs)\n",
    "        #print(dec_outputs.shape)#1,7,512\n",
    "        # dec_outputs: [batch_size，tgt_len,d_model]-> dec_logits: [batch_size,tgt_len，tgt_vocab_size]\n",
    "        #print(\"dec_outputs=\",dec_outputs.shape)\n",
    "        dec_logits = self.projection(dec_outputs)#1,7,11\n",
    "        \n",
    "        return dec_logits.view(-1,dec_logits.size(-1)),enc_self_attns,dec_self_attns,dec_enc_attns\n",
    "        #7,11\n",
    "\n",
    "\n",
    "model = Transformer().to(device)\n",
    "#这里的损失函数里面设置了一个参数ignore induxe4，因为“pad”这个单词的索引为0，这样设置以后，就不会计算“rod\"”的损失(因为本来“pad”也没有意义，不需要计印\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.SGD(model.parameters(),lr=1e-3,momentum=0.99)#用adam的话效果不好#= \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for enc_inputs, dec_inputs, dec_outputs in loader:   \n",
    "        #enc_inputs: [batch_size, src_len]\n",
    "        #dec_inputs: [batch_size,tgt_len]\n",
    "        #dec_outputs:[batcha_size,tgt_len]\n",
    "        enc_inputs,dec_inputs,dec_outputs = enc_inputs.to(device),dec_inputs.to(device),dec_outputs.to(device)\n",
    "        #outputs:[batch_size* tgt_len, tgt_vocab_size]\n",
    "        outputs,enc_self_attns,dec_self_attns,dec_enc_attns = model(enc_inputs,dec_inputs)\n",
    "        loss = criterion(outputs,dec_outputs.view(-1)) #dec_outputs.view(-1): [batch_size * tgt_len * tgt_vocab_size]\n",
    "        print('Epoch: ', '%04d' % (epoch +1),'loss =', '{:.6f}'.format(loss))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e049ff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoder(model, enc_input,start_symbol):\n",
    "    \"\"\"含心编码\n",
    "    For simplicity, a Greedy Decoder is Beam search when K=1，This is necessary for inference as we don 't know the\n",
    "    target sequence input，Therefore we try to generate the target input word by word， \n",
    "    then feed it into the transformer.Starting Reference: \n",
    "    http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding\n",
    "    : param model: Transformer Model\n",
    "    : param enc_input: The encoder input\n",
    "    : param start_symbol:The start symbol. In this example it is 's' which corresponds to index 4:\n",
    "    return: The target input\n",
    "    \"\"\"\n",
    "    #print(\"enc_input=\",enc_input.shape)\n",
    "    #print(\"start_symobl=\",start_symbol)\n",
    "    #enc_input= torch.Size([1, 8]) \n",
    "    #start_symobl= 8\n",
    "    #\n",
    "    enc_outputs, enc_self_attns = model.encoder(enc_input)\n",
    "    #print(\"enc_outputs=\",enc_outputs.shape)\n",
    "    #enc_outputs= torch.Size([1, 8, 512])\n",
    "    dec_input = torch.zeros(1, 0).type_as(enc_input.data)\n",
    "    #print(\"============greed========\")\n",
    "    #print(\"dec_input=\",dec_input.shape)#1,0\n",
    "    terminal = False\n",
    "    next_symbol = start_symbol\n",
    "    while not terminal:  \n",
    "        #预测阶段:dec_input序列会一点点变长（每次添加一个新预测出来的单词)\n",
    "        dec_input = torch.cat([dec_input.to(device),torch.tensor([[next_symbol]],dtype=enc_input.dtype).to(device)],-1)\n",
    "        #print(\"dec_input=\",dec_input.shape)#1,1\n",
    "        dec_outputs,_,_ = model.decoder(dec_input,enc_input,enc_outputs)\n",
    "        #print(\"dec_outpts=\",dec_outputs.shape)#1,1,512\n",
    "        projected =model.projection(dec_outputs)\n",
    "        #print(\"12t781t3781234t78134t73824\")\n",
    "        #print(\"projection\",projected.shape)\n",
    "        prob = projected.squeeze(0).max(dim=-1,keepdim=False)[1]\n",
    "         #增量更新(我们希望重复单词预测结果是一样的)\n",
    "         #我们在预测是会选择性忽略重复的预测的词，只摘取最新预测的单词拼接到输入序列中\n",
    "        next_word = prob.data[-1]#拿出当前预测的单词(数字)。我们用×'_t对应的输出z_t去预测下一个单词的概率，不用Z_1,7_2..z_ft-1)\n",
    "        next_symbol = next_word\n",
    "        if next_symbol==tgt_vocab[\"E\"]:\n",
    "            terminal = True\n",
    "            #print(next_word)    \n",
    "        #greedy_dec _predict = torch.cat(\n",
    "        #[dec_input.to(device)，torch.tensor( [[next_symbol]],dtype=enc_input.dtype).to(device)],#-1)\n",
    "    greedy_dec_predict = dec_input[:,1:]\n",
    "    return greedy_dec_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5207db4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 8, 4, 9, 6, 7, 0]) -> tensor([ 1,  2,  6,  7,  5, 10])\n",
      "['我', '有', '零', '个', '女', '朋', '友', 'p'] -> ['i', 'have', 'zero', 'girl', 'friend', '.']\n"
     ]
    }
   ],
   "source": [
    "enc_inputs, _,_=next(iter(loader))\n",
    "for i in range(len(enc_inputs)):\n",
    "    greedy_dec_predict = greedy_decoder(model,enc_inputs[i].view(1,-1).to(device),start_symbol=tgt_vocab[\"S\"])\n",
    "    print(enc_inputs[i],'->', greedy_dec_predict.squeeze())\n",
    "    print([src_idx2word[t.item()] for t in enc_inputs[i]],'->',[idx2word[n.item()] for n in greedy_dec_predict.squeeze()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3625f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ba1e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda (python-torch)",
   "language": "python",
   "name": "python-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
